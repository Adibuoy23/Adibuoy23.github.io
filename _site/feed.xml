<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-12-30T00:11:18+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Aditya Upadhyayula</title><subtitle>Powered by Jekyll and Github</subtitle><author><name>Aditya Upadhyayula, PhD</name><email>aditya.usa8@gmail.com</email></author><entry><title type="html">.Update?</title><link href="http://localhost:4000/update/" rel="alternate" type="text/html" title=".Update?" /><published>2023-12-29T00:00:00+05:30</published><updated>2023-12-29T00:00:00+05:30</updated><id>http://localhost:4000/update</id><content type="html" xml:base="http://localhost:4000/update/">&lt;p&gt;After a long hiatus, I was finally able to update my webpage.&lt;/p&gt;

&lt;p&gt;Few things happened in the meantime:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Our paper titled “Eccentricity advances arrival to perception” got out @ &lt;strong&gt;&lt;em&gt;JEP:G&lt;/em&gt;&lt;/strong&gt;. Here, we investigated how a visual moment is perceived in time. We found that two objectively simultaneously are not perceived that way across the fovea and periphery. Specifically, peripheral instants are perceived as occuring earlier compared to the foveal events. Read the paper to know more &lt;a href=&quot;https://adibuoy23.github.io/others/Eccentricity_paper.pdf&quot;&gt;…&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Our paper titled “Spatiotemporal jump detection during continuous film viewing” got out @ &lt;strong&gt;&lt;em&gt;JOV&lt;/em&gt;&lt;/strong&gt;. Here, we investigated how sensitive people are to disruptions when viewing naturalistic films. Participants watched 1min clips from the movie 1917 while being eye tracked. During an occasional saccade, the videos jumped either forward or backward in time. Using this novel temporal change blindness paradigm, we showed that participants are more sensitive to backward jumps more than forward jumps. We think knowledge about unfolding temporal information facilitates temporal visual processing, and could help with the jump detection. Read the paper to know more &lt;a href=&quot;https://adibuoy23.github.io/others/Spatiotemporal_saccade_paper.pdf&quot;&gt;…&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Our paper titled “Spatiotemporal jump detections during continuous film viewing: Insights from a flicker paradigm” just got accepted at &lt;strong&gt;&lt;em&gt;AP&amp;amp;P&lt;/em&gt;&lt;/strong&gt;. This paper established that the sensitivity during jumps in the above paper is not tied to the eye movements themselves. We showed this using a novel temporal change blindness flicker paradigm. I will link the paper here as soon as it gets out.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Oh, I also got a new job @ WashU. That meant another fun cross country trip!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I will try to post more regularly going forward. Something exciting is brewing. Watch this space!&lt;/p&gt;</content><author><name>Aditya Upadhyayula, PhD</name><email>aditya.usa8@gmail.com</email></author><summary type="html">After a long hiatus, I was finally able to update my webpage.</summary></entry><entry><title type="html">Python tutorials on introduction to algorithms</title><link href="http://localhost:4000/Intro-to-Algorithms/" rel="alternate" type="text/html" title="Python tutorials on introduction to algorithms" /><published>2020-12-16T00:00:00+05:30</published><updated>2020-12-16T00:00:00+05:30</updated><id>http://localhost:4000/Intro%20to%20Algorithms</id><content type="html" xml:base="http://localhost:4000/Intro-to-Algorithms/">&lt;p class=&quot;message&quot;&gt;
Recently, I have been spending some time learning about algorithms and data structures. These are particularly useful if you want to:
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Prepare for interviews with Google, FB, Apple, Microsoft, Amazon etc.&lt;/li&gt;
  &lt;li&gt;Write optimal code. [for eg: reducing complexity from O(n) to O(log n)]&lt;/li&gt;
  &lt;li&gt;Learn Reinforcement Learning Algorithms&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is the &lt;a href=&quot;https://github.com/Adibuoy23/Algorithms/&quot;&gt;github repo&lt;/a&gt;&lt;/p&gt;</content><author><name>Aditya Upadhyayula, PhD</name><email>aditya.usa8@gmail.com</email></author><category term="Tree search" /><category term="BFS" /><category term="DFS" /><category term="Dynamic Programming" /><category term="Python" /><category term="Jupyter notebook" /><category term="Google colab" /><summary type="html">Recently, I have been spending some time learning about algorithms and data structures. These are particularly useful if you want to: Prepare for interviews with Google, FB, Apple, Microsoft, Amazon etc. Write optimal code. [for eg: reducing complexity from O(n) to O(log n)] Learn Reinforcement Learning Algorithms</summary></entry><entry><title type="html">Our paper is out</title><link href="http://localhost:4000/Our-paper-is-out/" rel="alternate" type="text/html" title="Our paper is out" /><published>2020-09-16T00:00:00+05:30</published><updated>2020-09-16T00:00:00+05:30</updated><id>http://localhost:4000/Our%20paper%20is%20out</id><content type="html" xml:base="http://localhost:4000/Our-paper-is-out/">&lt;p class=&quot;message&quot;&gt;
I am excited to share that our paper exploring individual differences in Multiple Object Tracking (MOT) is out. I am looking forward to hearing your thoughts on this!


&lt;/p&gt;

&lt;p&gt;Here is the &lt;a href=&quot;https://adibuoy23.github.io/others/MOT_paper.pdf&quot;&gt;pdf&lt;/a&gt; of the same&lt;/p&gt;</content><author><name>Aditya Upadhyayula, PhD</name><email>aditya.usa8@gmail.com</email></author><category term="Multiple Object Tracking" /><category term="Individual Differences" /><category term="Eye Tracking" /><category term="Computational modelling" /><category term="Kalman Filter" /><summary type="html">I am excited to share that our paper exploring individual differences in Multiple Object Tracking (MOT) is out. I am looking forward to hearing your thoughts on this!</summary></entry><entry><title type="html">Our talk at CogSci 2020</title><link href="http://localhost:4000/talk@CogSci2020/" rel="alternate" type="text/html" title="Our talk at CogSci 2020" /><published>2020-07-22T00:00:00+05:30</published><updated>2020-07-22T00:00:00+05:30</updated><id>http://localhost:4000/talk@CogSci2020</id><content type="html" xml:base="http://localhost:4000/talk@CogSci2020/">&lt;p class=&quot;message&quot;&gt;
I am excited to share my research on analyzing visual narratives using computational psycholinguistics approaches. Do we group temporal events according to an underlying structure? Take reading comprehension task for example. Evidence from psycholinguistics shows that an underlying syntax structure is influencing our reading comprehension. Words that violate syntax structure require more processing. This hints at the possibility of grouping temporal events in time. Is this ability to group temporal events a general purpose mechanism? If yes, can it be applied to other cognitive processes as well? In this talk, I explore if our visual narrative comprehension is structured like spoken language? I demonstrate that we can use existing computational psycholingustics tools to study this question. This is a collaboration with Neil Cohn, PhD. I am looking forward to hearing your thoughts on this project!


&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/eEBSmQwxVmk&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name>Aditya Upadhyayula, PhD</name><email>aditya.usa8@gmail.com</email></author><category term="Visual Narratives" /><category term="Computational Psycholinguistics" /><category term="Hierarhical stucture in temporal processing" /><summary type="html">I am excited to share my research on analyzing visual narratives using computational psycholinguistics approaches. Do we group temporal events according to an underlying structure? Take reading comprehension task for example. Evidence from psycholinguistics shows that an underlying syntax structure is influencing our reading comprehension. Words that violate syntax structure require more processing. This hints at the possibility of grouping temporal events in time. Is this ability to group temporal events a general purpose mechanism? If yes, can it be applied to other cognitive processes as well? In this talk, I explore if our visual narrative comprehension is structured like spoken language? I demonstrate that we can use existing computational psycholingustics tools to study this question. This is a collaboration with Neil Cohn, PhD. I am looking forward to hearing your thoughts on this project!</summary></entry><entry><title type="html">Our poster at V-VSS 2020</title><link href="http://localhost:4000/poster@VSS_2020/" rel="alternate" type="text/html" title="Our poster at V-VSS 2020" /><published>2020-06-20T00:00:00+05:30</published><updated>2020-06-20T00:00:00+05:30</updated><id>http://localhost:4000/poster@VSS_2020</id><content type="html" xml:base="http://localhost:4000/poster@VSS_2020/">&lt;p class=&quot;message&quot;&gt;
Our experience of passage of time varies with the situation that we are in. For example, we report time being slowed down in near death experiences. In the cognitive psychology literature, this phenomenon is referred to as the subjective expansion of time. I along with Ian Phillips &amp;amp; Jon Flombaum show that the subjective expansion of time happens in our immediate memory, and not in our perceptual experience. In other words, these results suggest that our experience of time does not change in different situations. Instead, the way we remember how long those situations lasted for is distorted. I am looking forward to hearing your thoughts on this!

&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/w82668xFLfg&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name>Aditya Upadhyayula, PhD</name><email>aditya.usa8@gmail.com</email></author><category term="Subjective expansion of time" /><category term="perception" /><category term="memory" /><category term="vision" /><summary type="html">Our experience of passage of time varies with the situation that we are in. For example, we report time being slowed down in near death experiences. In the cognitive psychology literature, this phenomenon is referred to as the subjective expansion of time. I along with Ian Phillips &amp;amp; Jon Flombaum show that the subjective expansion of time happens in our immediate memory, and not in our perceptual experience. In other words, these results suggest that our experience of time does not change in different situations. Instead, the way we remember how long those situations lasted for is distorted. I am looking forward to hearing your thoughts on this!</summary></entry><entry><title type="html">Our talk at V-VSS 2020</title><link href="http://localhost:4000/talk@VSS_2020/" rel="alternate" type="text/html" title="Our talk at V-VSS 2020" /><published>2020-06-20T00:00:00+05:30</published><updated>2020-06-20T00:00:00+05:30</updated><id>http://localhost:4000/talk@VSS_2020</id><content type="html" xml:base="http://localhost:4000/talk@VSS_2020/">&lt;p class=&quot;message&quot;&gt;
I am excited to share my research on how space and time dissociate in the construction of a visual moment. This work - in collaboration with Ian Phillips and my advisor Jon Flombaum - suggests that a moment in our visual experience is a snapshot that is stitched together from different moments in time across the visual field. I am looking forward to hearing your thoughts on this!

&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/JQlGu8vNaOw&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name>Aditya Upadhyayula, PhD</name><email>aditya.usa8@gmail.com</email></author><category term="Visual moment" /><category term="Differntial processing of Fovea &amp; Periphery" /><category term="RSVP" /><category term="Awareness" /><summary type="html">I am excited to share my research on how space and time dissociate in the construction of a visual moment. This work - in collaboration with Ian Phillips and my advisor Jon Flombaum - suggests that a moment in our visual experience is a snapshot that is stitched together from different moments in time across the visual field. I am looking forward to hearing your thoughts on this!</summary></entry><entry><title type="html">.Hallo world?</title><link href="http://localhost:4000/hallo-world/" rel="alternate" type="text/html" title=".Hallo world?" /><published>2020-06-19T00:00:00+05:30</published><updated>2020-06-19T00:00:00+05:30</updated><id>http://localhost:4000/hallo-world</id><content type="html" xml:base="http://localhost:4000/hallo-world/">&lt;p&gt;I’m trying something new, shifting from vanilla &lt;a href=&quot;https://html5up.net/&quot;&gt;html5up&lt;/a&gt; to &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt;. This helps in a few areas: one is dynamic content updates, with the static blog-like format that Jekyll provides, along with a greater usage of markdown in the whole rendering process. My apologies if the website is less user-friendly for a while until I get all the kinks sorted out.&lt;/p&gt;

&lt;p&gt;I’m currently building on the &lt;a href=&quot;https://github.com/mmistakes/minimal-mistakes&quot;&gt;Minimal mistakes&lt;/a&gt; based on &lt;a href=&quot;https://zenglix.github.io/personal_website/&quot;&gt;Dr. Li Zeng’s&lt;/a&gt;, and &lt;a href=&quot;https://kartheikiyer.github.io&quot;&gt;Dr. Kartheik Iyer’s&lt;/a&gt;&lt;/p&gt;</content><author><name>Aditya Upadhyayula, PhD</name><email>aditya.usa8@gmail.com</email></author><summary type="html">I’m trying something new, shifting from vanilla html5up to Jekyll. This helps in a few areas: one is dynamic content updates, with the static blog-like format that Jekyll provides, along with a greater usage of markdown in the whole rendering process. My apologies if the website is less user-friendly for a while until I get all the kinks sorted out.</summary></entry></feed>